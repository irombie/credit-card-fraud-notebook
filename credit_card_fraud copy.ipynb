{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_curve, auc\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(credit_data.info())\n",
    "credit_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(credit_data.skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = credit_data.drop('Class', axis=1)\n",
    "y = credit_data['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g= sns.histplot(credit_data[\"Class\"])\n",
    "g.set_xticks([0,1])\n",
    "# g.set_xticklabels([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_hist_plot(df_train,df_test, df_train_label, df_test_label, feature):\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 12))\n",
    "\n",
    "    sns.histplot(data = df_train , ax=axes[0], x = feature , hue = df_train_label,palette = sns.color_palette([\"yellow\" , \"green\",'red','blue','black','orange','purple']) ,multiple = \"stack\" ).set_title(f\"{feature} Vs \")\n",
    "    axes[0].set_title('Histogram of Train Data '+feature)\n",
    "\n",
    "    sns.histplot(data = df_test , ax=axes[1], x = feature , hue = df_test_label,palette = sns.color_palette([\"yellow\" , \"green\",'red','blue','black','orange','purple']) ,multiple = \"stack\" ).set_title(f\"{feature} Vs \")\n",
    "    axes[1].set_title('Histogram of Test Data '+feature)\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=X_train.columns.tolist()\n",
    "for i in cols:\n",
    "    multi_hist_plot(X_train,X_test, y_train, y_test, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation(X, y):\n",
    "    train_corrs = {}\n",
    "    for feature in X.columns:\n",
    "        train_corrs[feature] = pearsonr(X[feature], y)[0]\n",
    "    train_corrs = {k: v for k, v in sorted(train_corrs.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return train_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_correlation(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_correlation(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Impute missing values in the features\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Neural Network': MLPClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5  # Set a threshold to determine fraud (1) and non-fraud (0)\n",
    "y_train_binary = (y_train > threshold).astype(int)\n",
    "\n",
    "# Model training and evaluation\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Fit the model to the training data with binary labels\n",
    "    model.fit(X_train_scaled, y_train_binary)\n",
    "\n",
    "    if isinstance(model, SVC) and not model.probability:\n",
    "        # Use decision_function instead\n",
    "        y_pred_scores = model.decision_function(X_test_scaled)\n",
    "        y_pred_proba = (y_pred_scores - y_pred_scores.min()) / (y_pred_scores.max() - y_pred_scores.min())  # Normalize scores\n",
    "    else:\n",
    "        # Use predict_proba for other models\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "\n",
    "    results[name] = {'confusion_matrix': cm, 'precision_recall_auc': pr_auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {}\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies[name] = accuracy\n",
    "\n",
    "print(\"Model Accuracies:\")\n",
    "for name, accuracy in accuracies.items():\n",
    "    print(f\"{name}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    print(f\"Confusion matrix results for {result} are \\n{results[result]['confusion_matrix']}\")\n",
    "    print(f\"AUC for {result} is {results[result]['precision_recall_auc']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
